{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"C:\\\\Users\\\\mjbab\\\\OneDrive\\\\Desktop\\\\df_reduced.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=\";\", low_memory=False)\n",
    "\n",
    "# Create a group_ids column based on Info_cluster\n",
    "df['group_ids'] = df.groupby('Info_cluster').ngroup()\n",
    "\n",
    "# Check the distribution of group_ids\n",
    "group_distribution = df['group_ids'].value_counts()\n",
    "\n",
    "# Ensure each group has sufficient samples for GroupKFold\n",
    "min_group_samples = 10  # Adjust as needed\n",
    "valid_groups = group_distribution[group_distribution >= min_group_samples].index\n",
    "df_valid_groups = df[df['group_ids'].isin(valid_groups)]\n",
    "\n",
    "# EDA Part\n",
    "# A) Summary statistics for numerical features\n",
    "summary_stats = df_valid_groups.describe()\n",
    "\n",
    "# C) Handling missing values\n",
    "# Drop non-numeric columns\n",
    "non_numeric_columns = ['Info_PepID', 'Info_organism_id', 'Info_protein_id', 'Info_pos', 'Info_AA'\n",
    "                       , 'Info_epitope_id', 'Info_nPos', 'Info_nNeg']\n",
    "df_numeric = df_valid_groups.drop(columns=non_numeric_columns)\n",
    "\n",
    "# Impute missing values (using IterativeImputer)\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "\n",
    "# D) Outlier detection using IQR method\n",
    "def detect_outliers_iqr(data):\n",
    "    outliers_indices = []\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype != 'object':  # Exclude non-numeric columns\n",
    "            Q1 = np.percentile(data[column], 25)\n",
    "            Q3 = np.percentile(data[column], 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers_indices.extend(data[(data[column] < lower_bound) | (data[column] > upper_bound)].index)\n",
    "    return list(set(outliers_indices))\n",
    "\n",
    "outliers_indices = detect_outliers_iqr(df_imputed)\n",
    "\n",
    "# Filter outliers_indices to contain only indices present in df_valid_groups\n",
    "outliers_indices_filtered = [idx for idx in outliers_indices if idx in df_valid_groups.index]\n",
    "\n",
    "# Remove outliers from df_valid_groups\n",
    "df_valid_groups_no_outliers = df_valid_groups.drop(outliers_indices_filtered)\n",
    "\n",
    "# Check if the length of df_valid_groups_no_outliers matches the length of df_valid_groups\n",
    "if len(df_valid_groups_no_outliers) != len(df_valid_groups):\n",
    "    print(\"Length mismatch between df_valid_groups_no_outliers and df_valid_groups. Adjusting dimensions...\")\n",
    "\n",
    "    # If df_valid_groups_no_outliers is longer than df_valid_groups, trim its rows\n",
    "    if len(df_valid_groups_no_outliers) > len(df_valid_groups):\n",
    "        df_valid_groups_no_outliers = df_valid_groups_no_outliers.iloc[:len(df_valid_groups), :]\n",
    "\n",
    "    # If df_valid_groups_no_outliers is shorter than df_valid_groups, expand its rows\n",
    "    elif len(df_valid_groups_no_outliers) < len(df_valid_groups):\n",
    "        # Determine the number of rows to add\n",
    "        num_rows_to_add = len(df_valid_groups) - len(df_valid_groups_no_outliers)\n",
    "        \n",
    "        # Create a DataFrame with the same columns as df_valid_groups_no_outliers and fill it with NaNs\n",
    "        new_rows_df = pd.DataFrame(np.nan, index=np.arange(num_rows_to_add), columns=df_valid_groups_no_outliers.columns)\n",
    "        \n",
    "        # Concatenate df_valid_groups_no_outliers with the new rows DataFrame\n",
    "        df_valid_groups_no_outliers = pd.concat([df_valid_groups_no_outliers, new_rows_df], ignore_index=True)\n",
    "\n",
    "# E) Class distribution\n",
    "class_distribution = df_valid_groups_no_outliers['Class'].value_counts()\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Class', data=df_valid_groups_no_outliers)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_valid_groups_no_outliers['Class'] = label_encoder.fit_transform(df_valid_groups_no_outliers['Class'])\n",
    "\n",
    "# Create a synthetic minority class if there's only one class\n",
    "unique_classes = df_valid_groups_no_outliers['Class'].unique()\n",
    "if len(unique_classes) == 1:\n",
    "    # Duplicate a small portion of the existing class instances\n",
    "    minority_class_size = int(0.1 * len(df_valid_groups_no_outliers))\n",
    "    synthetic_minority = df_valid_groups_no_outliers.sample(n=minority_class_size, random_state=42)\n",
    "    synthetic_minority['Class'] = 1  # Assign the synthetic minority class label\n",
    "    df_valid_groups_no_outliers = pd.concat([df_valid_groups_no_outliers, synthetic_minority], ignore_index=True)\n",
    "    df_valid_groups_no_outliers['group_ids'] = df_valid_groups_no_outliers.groupby('Info_cluster').ngroup().values\n",
    "\n",
    "# Data Preprocessing\n",
    "# Handling missing values and ensuring numeric columns\n",
    "# Drop non-numeric columns\n",
    "non_numeric_columns = ['Info_PepID', 'Info_organism_id', 'Info_protein_id', 'Info_pos', 'Info_AA', 'Info_epitope_id', 'Info_nPos', 'Info_nNeg']\n",
    "df_numeric = df_valid_groups_no_outliers.drop(columns=non_numeric_columns)\n",
    "\n",
    "# Convert non-numeric columns to numeric (if possible)\n",
    "for col in df_numeric.columns:\n",
    "    df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n",
    "\n",
    "# Impute missing values (using IterativeImputer)\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "\n",
    "# Define X_imputed after imputation\n",
    "X_imputed = df_imputed.drop(columns=['Class'])\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_imputed)\n",
    "\n",
    "# Check for missing values in the 'group_ids' column\n",
    "missing_group_ids = df_valid_groups_no_outliers['group_ids'].isnull().sum()\n",
    "\n",
    "# Drop rows with missing values in the 'group_ids' column\n",
    "if missing_group_ids > 0:\n",
    "    print(\"Dropping rows with missing 'group_ids'...\")\n",
    "    df_valid_groups_no_outliers = df_valid_groups_no_outliers.dropna(subset=['group_ids'])\n",
    "    print(\"Rows with missing 'group_ids' dropped.\")\n",
    "else:\n",
    "    print(\"No missing values found in 'group_ids' column.\")\n",
    "\n",
    "# Ensure consistent lengths for GroupKFold.split()\n",
    "X_reduced = X_reduced[:len(df_valid_groups_no_outliers)]\n",
    "df_valid_groups_no_outliers = df_valid_groups_no_outliers.iloc[:len(X_reduced)]\n",
    "\n",
    "# Initialize GroupKFold with the correct number of groups\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Report accuracy for each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    balanced_accuracies = []\n",
    "    for train_index, test_index in group_kfold.split(X_reduced, df_valid_groups_no_outliers['Class'], groups=df_valid_groups_no_outliers['group_ids']):\n",
    "        X_train, X_test = X_reduced[train_index], X_reduced[test_index]\n",
    "        y_train, y_test = df_valid_groups_no_outliers['Class'].iloc[train_index], df_valid_groups_no_outliers['Class'].iloc[test_index]\n",
    "\n",
    "        # Apply SMOTE within each fold\n",
    "        smote = SMOTE()\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Train the model\n",
    "        clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        balanced_accuracies.append(balanced_accuracy)\n",
    "\n",
    "        # Visualize confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "        plt.title(f'{name} Classifier Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "\n",
    "    # Report performance\n",
    "    print(f'Classifier: {name}')\n",
    "    print(f'Mean Balanced Accuracy: {np.mean(balanced_accuracies)}')\n",
    "\n",
    "# Load the holdout dataset\n",
    "holdout_file_path = \"C:\\\\Users\\\\mjbab\\\\OneDrive\\\\Desktop\\\\df_reduced_holdout.csv\"\n",
    "holdout_df = pd.read_csv(holdout_file_path, delimiter=\";\", low_memory=False)\n",
    "\n",
    "# Create 'group_ids' column for holdout dataset based on the same logic as training dataset\n",
    "holdout_df['group_ids'] = holdout_df.groupby('Info_cluster').ngroup()\n",
    "\n",
    "# Initialize an empty list to store group-wise predictions\n",
    "holdout_group_predictions = []\n",
    "\n",
    "# Iterate over unique group_ids in the holdout dataset\n",
    "holdout_group_ids = holdout_df['group_ids'].unique()\n",
    "\n",
    "for group_id in holdout_group_ids:\n",
    "    # Filter holdout data for the current group\n",
    "    group_data = holdout_df[holdout_df['group_ids'] == group_id]\n",
    "\n",
    "    # Extract features for the group (similar to preprocessing steps)\n",
    "    non_numeric_columns = ['Info_PepID', 'Info_organism_id', 'Info_protein_id', 'Info_pos', 'Info_AA', 'Info_epitope_id', 'Info_nPos', 'Info_nNeg']\n",
    "    group_numeric = group_data.drop(columns=non_numeric_columns)\n",
    "\n",
    "    # Impute missing values (using the same IterativeImputer as in training)\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    group_imputed = pd.DataFrame(imputer.fit_transform(group_numeric), columns=group_numeric.columns)\n",
    "\n",
    "    # Perform dimensionality reduction using PCA (reduce to 1 component)\n",
    "    pca = PCA(n_components=1)\n",
    "    group_X_reduced = pca.fit_transform(group_imputed)\n",
    "\n",
    "    # Ensure the shape of group_X_reduced is (n_samples, 1)\n",
    "    print(\"Shape of group_X_reduced:\", group_X_reduced.shape)  # Debugging output\n",
    "\n",
    "    # Make predictions for the group using the trained classifier (clf)\n",
    "    group_predictions = clf.predict(group_X_reduced)\n",
    "    # Store the group predictions along with the group_ids\n",
    "    holdout_group_predictions.extend(zip(group_data.index, group_predictions))\n",
    "\n",
    "# Create a DataFrame from the group-wise predictions\n",
    "holdout_predictions_df = pd.DataFrame(holdout_group_predictions, columns=['Index', 'Predicted_Class'])\n",
    "\n",
    "# Merge predictions with the original holdout data based on the index\n",
    "holdout_predictions_merged = pd.merge(holdout_df, holdout_predictions_df, left_index=True, right_on='Index', how='left')\n",
    "\n",
    "# Drop the extra index column and save the predictions to a CSV file\n",
    "holdout_predictions_merged.drop(columns='Index', inplace=True)\n",
    "# Specify the file path for saving the CSV file on the desktop\n",
    "desktop_path = \"C:\\\\Users\\\\mjbab\\\\OneDrive\\\\Desktop\\\\holdout_predictions_supervised.csv\"\n",
    "holdout_predictions_merged.to_csv(desktop_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
